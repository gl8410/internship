{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    处理文件，读入，配对并保存到一个数据集\n",
    "\n",
    "\"\"\"\n",
    "# 在当前目录下定义的数据集文件夹\n",
    "DataDir = \"Proteinfasta/\"\n",
    "\"\"\"\n",
    "    其他的随便写点什么\n",
    "\n",
    "\"\"\"\n",
    "DataPairs=[\n",
    "    [\"CF_TR_Sequence.fasta.csv\",\"CF_TE_Sequence.fasta.csv\"],\n",
    "    [\"CRYS_TR_Sequence.fasta.csv\",\"CRYS_TE_Sequence.fasta.csv\"],\n",
    "    [\"MC_TR_Sequence.fasta.csv\",\"MC_TE_Sequence.fasta.csv\"],\n",
    "    [\"MF_TR_Sequence.fasta.csv\",\"MF_TE_Sequence.fasta.csv\"],\n",
    "    [\"PF_TR_Sequence.fasta.csv\",\"PF_TE_Sequence.fasta.csv\"],\n",
    "    [\"Combined_TrainSet.csv\",\"Combined_TestSet.csv\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "ModelDir= \"Models/bi-LSTM-all_l1_e2_ba32.pth\" # 在当前目录下定义的模型集文件夹\n",
    "Set2Train= DataPairs[5] # 要训练的数据集\n",
    "vocab_size = 100   #词表大小\n",
    "embedding_size = 32   #词向量维度\n",
    "num_classes = 2    #二分类\n",
    "sentence_max_len = 1000  #单个句子的长度\n",
    "hidden_size = 16\n",
    "\n",
    "num_layers = 1  #一层lstm\n",
    "num_directions = 2  #双向lstm\n",
    "lr = 1e-3\n",
    "batch_size = 8   \n",
    "epochs = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "这里进行数据探索\n",
    "\n",
    "\"\"\"\n",
    "def Load_Exploring(pair,pos,length:int=1000):\n",
    "    df=pd.read_csv(DataDir+pair[pos])\n",
    "    df[\"Length\"]=df[\"Text\"].apply(len)\n",
    "    print(\"Data set is %s \" % pair[pos])\n",
    "    print(\"Max length of Text on this data set: \"+ str(df[\"Length\"].max()))\n",
    "    print(\"Labels are: \"+ str(df[\"y\"].unique()))\n",
    "    df[\"Length\"].plot(kind=\"hist\",bins=50,title=pair[pos]+\" Length distribution\")\n",
    "    # for d in df.groupby([\"y\"]):\n",
    "    #     d[1].plot(kind=\"hist\",bins=100,title=str(d[0])+ \" Length distribution\")\n",
    "    #     print(str(d[0])+\" shape is \"+ str(d[1].shape))\n",
    "    print(\"{0}  of the proteins have a Length less than {1}\".format(round(((df[df[\"Length\"]<length].shape[0]/df.shape[0])*100),2),length))\n",
    "    \n",
    "# Params: {0}:dataset {1}:train or test set {2}:length of the data to explore\n",
    "# Load_Exploring(DataPairs[0],0,1000)\n",
    "# Load_Exploring(DataPairs[1],0,1000)\n",
    "# Load_Exploring(DataPairs[2],0,1000)\n",
    "# Load_Exploring(DataPairs[3],0,1000)\n",
    "# Load_Exploring(DataPairs[4],0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of X,y in Train Set:\n",
      "0    0.794629\n",
      "1    0.205371\n",
      "Name: y, dtype: float64\n",
      "Num of TrainSet: 53245\n",
      "Proportion of X,y in Test Set:\n",
      "0    0.796378\n",
      "1    0.203622\n",
      "Name: y, dtype: float64\n",
      "Num of TestSet: 13309\n",
      "Prop of Train:Test is 0.8000270457072453:0.19997295429275475\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here exploring the data set and trying to get balanced data for training\n",
    "\n",
    "\"\"\"\n",
    "dftr=pd.read_csv(DataDir+Set2Train[0])\n",
    "print(\"Proportion of X,y in Train Set:\")\n",
    "print(dftr[\"y\"].value_counts()/len(dftr))\n",
    "print(\"Num of TrainSet: %s\" % len(dftr))\n",
    "dfte=pd.read_csv(DataDir+Set2Train[1])\n",
    "print(\"Proportion of X,y in Test Set:\")\n",
    "print(dfte[\"y\"].value_counts()/len(dfte))\n",
    "print(\"Num of TestSet: %s\" % len(dfte))\n",
    "print(\"Prop of Train:Test is {0}:{1}\".format(len(dftr)/(len(dftr)+len(dfte)),len(dfte)/(len(dftr)+len(dfte))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here Define the data loader device\n",
    "\n",
    "\"\"\"\n",
    "def Character2Sent(chars):\n",
    "    out=\"\"\n",
    "    for c in chars:\n",
    "        out=out+c+\" \"\n",
    "    return out[:-1]\n",
    "\n",
    "class DataLoader(object):\n",
    "    def read_raw(self,file):\n",
    "        data=[]\n",
    "        label=[]\n",
    "        df=pd.read_csv(file)\n",
    "        data=df[\"Text\"].apply(Character2Sent)\n",
    "        label=df[\"y\"].apply(lambda x:[1,0] if x==1 else [0,1])\n",
    "        return list(data),list(label)\n",
    "\n",
    "    def word_count(self, datas):\n",
    "        #统计单词出现的频次，并将其降序排列，得出出现频次最多的单词\n",
    "        dic = {}\n",
    "        for data in datas:\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() #所有单词转化为小写\n",
    "                if(word in dic):\n",
    "                    dic[word] += 1\n",
    "                else:\n",
    "                    dic[word] = 1\n",
    "        word_count_sorted = sorted(dic.items(), key=lambda item:item[1], reverse=True)\n",
    "        return  word_count_sorted\n",
    "\n",
    "    def word_index(self, datas, vocab_size):\n",
    "        #创建词表\n",
    "        word_count_sorted = self.word_count(datas)\n",
    "        word2index = {}\n",
    "        #词表中未出现的词\n",
    "        word2index[\"<unk>\"] = 0\n",
    "        #句子添加的padding\n",
    "        word2index[\"<pad>\"] = 1\n",
    "        #词表的实际大小由词的数量和限定大小决定\n",
    "        vocab_size = min(len(word_count_sorted), vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            word = word_count_sorted[i][0]\n",
    "            word2index[word] = i + 2\n",
    "          \n",
    "        return word2index, vocab_size\n",
    "\n",
    "    def get_datasets(self,data_set,vocab_size, embedding_size, max_len):\n",
    "        #注，由于nn.Embedding每次生成的词嵌入不固定，因此此处同时获取训练数据的词嵌入和测试数据的词嵌入\n",
    "        #测试数据的词表也用训练数据创建\n",
    "        train_datas, train_labels = self.read_raw(DataDir+data_set[0])\n",
    "        word2index, vocab_size = self.word_index(train_datas, vocab_size)\n",
    "        test_datas, test_labels = self.read_raw(DataDir+data_set[1])\n",
    "        \n",
    "        train_features = []\n",
    "        for data in train_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() #词表中的单词均为小写\n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"]) #词表中未出现的词用<unk>代替\n",
    "                if(len(feature)==max_len): #限制句子的最大长度，超出部分直接截断\n",
    "                    break\n",
    "            #对未达到最大长度的句子添加padding\n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            train_features.append(feature)\n",
    "            \n",
    "        test_features = []\n",
    "        for data in test_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() #词表中的单词均为小写\n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"]) #词表中未出现的词用<unk>代替\n",
    "                if(len(feature)==max_len): #限制句子的最大长度，超出部分直接截断\n",
    "                    break\n",
    "            #对未达到最大长度的句子添加padding\n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            test_features.append(feature)\n",
    "            \n",
    "        #将词的index转换成tensor,train_features中数据的维度需要一致，否则会报错\n",
    "        train_features = torch.LongTensor(train_features)\n",
    "        train_labels = torch.FloatTensor(train_labels)\n",
    "        \n",
    "        test_features = torch.LongTensor(test_features)\n",
    "        test_labels = torch.FloatTensor(test_labels)\n",
    "        \n",
    "        #将词转化为embedding\n",
    "        #词表中有两个特殊的词<unk>和<pad>，所以词表实际大小为vocab_size + 2\n",
    "        embed = nn.Embedding(vocab_size + 2, embedding_size)\n",
    "        train_features = embed(train_features)\n",
    "        test_features = embed(test_features)\n",
    "        #指定输入特征是否需要计算梯度\n",
    "        train_features = Variable(train_features, requires_grad=False)\n",
    "        train_datasets = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "        \n",
    "        test_features = Variable(test_features, requires_grad=False)\n",
    "        test_datasets = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "        return train_datasets, test_datasets\n",
    "\n",
    "    def get_Features(self,data_set,vocab_size, embedding_size, max_len):\n",
    "        #注，由于nn.Embedding每次生成的词嵌入不固定，因此此处同时获取训练数据的词嵌入和测试数据的词嵌入\n",
    "        #测试数据的词表也用训练数据创建\n",
    "        train_datas, train_labels = self.read_raw(DataDir+data_set[0])\n",
    "        word2index, vocab_size = self.word_index(train_datas, vocab_size)\n",
    "        test_datas, test_labels = self.read_raw(DataDir+data_set[1])\n",
    "        \n",
    "        train_features = []\n",
    "        for data in train_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() #词表中的单词均为小写\n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"]) #词表中未出现的词用<unk>代替\n",
    "                if(len(feature)==max_len): #限制句子的最大长度，超出部分直接截断\n",
    "                    break\n",
    "            #对未达到最大长度的句子添加padding\n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            train_features.append(feature)\n",
    "            \n",
    "        test_features = []\n",
    "        for data in test_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() #词表中的单词均为小写\n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"]) #词表中未出现的词用<unk>代替\n",
    "                if(len(feature)==max_len): #限制句子的最大长度，超出部分直接截断\n",
    "                    break\n",
    "            #对未达到最大长度的句子添加padding\n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            test_features.append(feature)\n",
    "            \n",
    "        #将词的index转换成tensor,train_features中数据的维度需要一致，否则会报错\n",
    "        train_features = torch.LongTensor(train_features)\n",
    "        train_labels = torch.FloatTensor(train_labels)\n",
    "        \n",
    "        test_features = torch.LongTensor(test_features)\n",
    "        test_labels = torch.FloatTensor(test_labels)\n",
    "        \n",
    "        #将词转化为embedding\n",
    "        #词表中有两个特殊的词<unk>和<pad>，所以词表实际大小为vocab_size + 2\n",
    "        embed = nn.Embedding(vocab_size + 2, embedding_size)\n",
    "        train_features = embed(train_features)\n",
    "        test_features = embed(test_features)\n",
    "        #指定输入特征是否需要计算梯度\n",
    "        train_features = Variable(train_features, requires_grad=False)\n",
    "        \n",
    "        test_features = Variable(test_features, requires_grad=False)\n",
    "        return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_size,hidden_size, num_layers, num_directions, num_classes):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        \n",
    "        self.input_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = (num_directions == 2))\n",
    "        self.attention_weights_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.liner = nn.Linear(hidden_size, num_classes)\n",
    "        self.act_func = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #lstm的输入维度为 [seq_len, batch, input_size]\n",
    "        #x [batch_size, sentence_length, embedding_size]\n",
    "        x = x.permute(1, 0, 2)         #[sentence_length, batch_size, embedding_size]\n",
    "        #由于数据集不一定是预先设置的batch_size的整数倍，所以用size(1)获取当前数据实际的batch\n",
    "        batch_size = x.size(1)\n",
    "        \n",
    "        #设置lstm最初的前项输出\n",
    "        h_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        #out[seq_len, batch, num_directions * hidden_size]。多层lstm，out只保存最后一层每个时间步t的输出h_t\n",
    "        #h_n, c_n [num_layers * num_directions, batch, hidden_size]\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        #将双向lstm的输出拆分为前向输出和后向输出\n",
    "        (forward_out, backward_out) = torch.chunk(out, 2, dim = 2)\n",
    "        out = forward_out + backward_out  #[seq_len, batch, hidden_size]\n",
    "        out = out.permute(1, 0, 2)  #[batch, seq_len, hidden_size]\n",
    "        \n",
    "        #为了使用到lstm最后一个时间步时，每层lstm的表达，用h_n生成attention的权重\n",
    "        h_n = h_n.permute(1, 0, 2)  #[batch, num_layers * num_directions,  hidden_size]\n",
    "        h_n = torch.sum(h_n, dim=1) #[batch, 1,  hidden_size]\n",
    "        h_n = h_n.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        \n",
    "        attention_w = self.attention_weights_layer(h_n)  #[batch, hidden_size]\n",
    "        attention_w = attention_w.unsqueeze(dim=1) #[batch, 1, hidden_size]\n",
    "        \n",
    "        attention_context = torch.bmm(attention_w, out.transpose(1, 2))  #[batch, 1, seq_len]\n",
    "        softmax_w = F.softmax(attention_context, dim=-1)  #[batch, 1, seq_len],权重归一化\n",
    "        \n",
    "        x = torch.bmm(softmax_w, out)  #[batch, 1, hidden_size]\n",
    "        x = x.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        x = self.liner(x)\n",
    "        x = self.act_func(x)\n",
    "        return x\n",
    "        \n",
    "def test(model, test_loader, loss_func):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    corrects = 0.0\n",
    "    for datas, labels in test_loader:\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = model(datas)\n",
    "        loss = loss_func(preds, labels)\n",
    "        \n",
    "        loss_val += loss.item() * datas.size(0)\n",
    "        \n",
    "        #获取预测的最大概率出现的位置\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        corrects += torch.sum(preds == labels).item()\n",
    "    test_loss = loss_val / len(test_loader.dataset)\n",
    "    test_acc = corrects / len(test_loader.dataset)\n",
    "    print(\"Test Loss: {}, Test Acc: {}\".format(test_loss, test_acc))\n",
    "    return test_acc\n",
    "\n",
    "def train(model, train_loader,test_loader, optimizer, loss_func, epochs):\n",
    "    best_val_acc = 0.0\n",
    "    best_model_params = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_val = 0.0\n",
    "        corrects = 0.0\n",
    "        for datas, labels in train_loader:\n",
    "            datas = datas.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(datas)\n",
    "            loss = loss_func(preds, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_val += loss.item() * datas.size(0)\n",
    "            \n",
    "            #获取预测的最大概率出现的位置\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            corrects += torch.sum(preds == labels).item()\n",
    "        train_loss = loss_val / len(train_loader.dataset)\n",
    "        train_acc = corrects / len(train_loader.dataset)\n",
    "        if(epoch % 1 == 0):\n",
    "            print(f\"--------Epoch <<{epoch}>> Results:--------\")\n",
    "            print(\"Train Loss: {}, Train Acc: {}\".format(train_loss, train_acc))\n",
    "            test_acc = test(model, test_loader, loss_func)\n",
    "            if(best_val_acc < test_acc):\n",
    "                best_val_acc = test_acc\n",
    "                best_model_params = copy.deepcopy(model.state_dict())\n",
    "                print(\"Best model params updated!\")\n",
    "    model.load_state_dict(best_model_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dload=DataLoader()\n",
    "trSet,teSet=Dload.get_datasets(Set2Train,vocab_size=vocab_size,embedding_size=embedding_size,max_len=sentence_max_len)\n",
    "train_loader = torch.utils.data.DataLoader(trSet, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(teSet, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMModel(embedding_size, hidden_size, num_layers, num_directions, num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.BCELoss()\n",
    "model = train(model, train_loader, test_loader, optimizer, loss_func, epochs)\n",
    "torch.save(model,ModelDir)\n",
    "print(f\"Model saved to {ModelDir} !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L4,Epoch10,Batch32\n",
    "\n",
    "--------Epoch <<0>> Results:--------\n",
    "Train Loss: 0.4866435846987773, Train Acc: 0.7935768616771528\n",
    "Test Loss: 0.4605570047280944, Test Acc: 0.796378390562777\n",
    "Best model params updated!\n",
    "--------Epoch <<1>> Results:--------\n",
    "Train Loss: 0.4676954279137123, Train Acc: 0.7946286036247535\n",
    "Test Loss: 0.45619719377931944, Test Acc: 0.796378390562777\n",
    "--------Epoch <<2>> Results:--------\n",
    "Train Loss: 0.46273632308039514, Train Acc: 0.7946286036247535\n",
    "Test Loss: 0.4555758065326091, Test Acc: 0.796378390562777\n",
    "--------Epoch <<3>> Results:--------\n",
    "Train Loss: 0.45822107717569743, Train Acc: 0.794572260306132\n",
    "Test Loss: 0.4521541875388981, Test Acc: 0.7964535276880307\n",
    "Best model params updated!\n",
    "--------Epoch <<4>> Results:--------\n",
    "Train Loss: 0.4535511062966679, Train Acc: 0.7969011174758194\n",
    "Test Loss: 0.4536914214277995, Test Acc: 0.798933052821399\n",
    "Best model params updated!\n",
    "--------Epoch <<5>> Results:--------\n",
    "Train Loss: 0.45050139221767604, Train Acc: 0.7978589538923843\n",
    "Test Loss: 0.45004856531370485, Test Acc: 0.7992336013224134\n",
    "Best model params updated!\n",
    "--------Epoch <<6>> Results:--------\n",
    "Train Loss: 0.4475180268130937, Train Acc: 0.7998309700441356\n",
    "Test Loss: 0.4452496790992962, Test Acc: 0.7998346983244421\n",
    "Best model params updated!\n",
    "--------Epoch <<7>> Results:--------\n",
    "Train Loss: 0.44521234057544934, Train Acc: 0.8006385576110433\n",
    "Test Loss: 0.4503334618405653, Test Acc: 0.7983319558193703\n",
    "--------Epoch <<8>> Results:--------\n",
    "Train Loss: 0.4413476846130569, Train Acc: 0.8033054746924594\n",
    "Test Loss: 0.44496255158007625, Test Acc: 0.7979562701931024\n",
    "--------Epoch <<9>> Results:--------\n",
    "Train Loss: 0.4388651643675676, Train Acc: 0.80409428115316\n",
    "Test Loss: 0.44902093252265773, Test Acc: 0.7993838755729206\n",
    "Model saved to Models/bi-LSTM-all_l4_e10.pth !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,matthews_corrcoef,classification_report\n",
    "\n",
    "def GetPrediction(features,model_path,batch):\n",
    "    model = torch.load(model_path)\n",
    "    length=len(features)\n",
    "    c_ind=0\n",
    "    out=torch.Tensor()\n",
    "    out=out.cuda(0)\n",
    "    while(c_ind<length):\n",
    "        if length-c_ind>batch:\n",
    "            l=batch\n",
    "        else:\n",
    "            l=1\n",
    "        pred=model(features[c_ind:c_ind+l].cuda(0))\n",
    "        out=torch.cat((out,pred))\n",
    "        c_ind=c_ind+l\n",
    "    return out\n",
    "\n",
    "def EvaluatePair(Dset,result):\n",
    "    dfte=pd.read_csv(DataDir+\"/\"+Dset[1])\n",
    "    dfpred=pd.DataFrame(result,columns=[\"0\",\"1\"])\n",
    "    dfpred[\"Pred\"]=dfpred[\"0\"]<=dfpred[\"1\"]\n",
    "    dfpred[\"Pred\"]=dfpred[\"Pred\"].apply(lambda x:0 if x==True else 1)\n",
    "    cm=confusion_matrix(dfte[\"y\"],dfpred[\"Pred\"])\n",
    "    mc=matthews_corrcoef(dfte[\"y\"],dfpred[\"Pred\"])\n",
    "    target_names = ['No crystal', 'Crystal']\n",
    "    print(\"-------%s Metrics on test set are: \" % Dset[1])\n",
    "    print(\"----Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"----Matthews Corrcoef:\")\n",
    "    print(mc)\n",
    "    print(classification_report(dfte[\"y\"], dfpred[\"Pred\"], target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------CF_TE_Sequence.fasta.csv Metrics on test set are: \n",
      "----Confusion Matrix:\n",
      "[[ 66  77]\n",
      " [174 229]]\n",
      "----Matthews Corrcoef:\n",
      "0.026377304180641305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No crystal       0.28      0.46      0.34       143\n",
      "     Crystal       0.75      0.57      0.65       403\n",
      "\n",
      "    accuracy                           0.54       546\n",
      "   macro avg       0.51      0.51      0.50       546\n",
      "weighted avg       0.62      0.54      0.57       546\n",
      "\n",
      "-------CRYS_TE_Sequence.fasta.csv Metrics on test set are: \n",
      "----Confusion Matrix:\n",
      "[[3647  979]\n",
      " [ 276   45]]\n",
      "----Matthews Corrcoef:\n",
      "-0.04343660694468044\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No crystal       0.93      0.79      0.85      4626\n",
      "     Crystal       0.04      0.14      0.07       321\n",
      "\n",
      "    accuracy                           0.75      4947\n",
      "   macro avg       0.49      0.46      0.46      4947\n",
      "weighted avg       0.87      0.75      0.80      4947\n",
      "\n",
      "-------MC_TE_Sequence.fasta.csv Metrics on test set are: \n",
      "----Confusion Matrix:\n",
      "[[891   0]\n",
      " [129   0]]\n",
      "----Matthews Corrcoef:\n",
      "0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No crystal       0.87      1.00      0.93       891\n",
      "     Crystal       0.00      0.00      0.00       129\n",
      "\n",
      "    accuracy                           0.87      1020\n",
      "   macro avg       0.44      0.50      0.47      1020\n",
      "weighted avg       0.76      0.87      0.81      1020\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\nlp38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\nlp38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------MF_TE_Sequence.fasta.csv Metrics on test set are: \n",
      "----Confusion Matrix:\n",
      "[[ 456 3092]\n",
      " [ 201 1198]]\n",
      "----Matthews Corrcoef:\n",
      "-0.020106151333621612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No crystal       0.69      0.13      0.22      3548\n",
      "     Crystal       0.28      0.86      0.42      1399\n",
      "\n",
      "    accuracy                           0.33      4947\n",
      "   macro avg       0.49      0.49      0.32      4947\n",
      "weighted avg       0.58      0.33      0.27      4947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in DataPairs[0:4]:\n",
    "    Dload1=DataLoader()\n",
    "    trFeatures,teFeatures=Dload1.get_Features(d,vocab_size=vocab_size,embedding_size=embedding_size,max_len=sentence_max_len)\n",
    "    result=GetPrediction(teFeatures,\"Models/bi-LSTM-all_l4_e10.pth\",100)\n",
    "    EvaluatePair(d,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------CF_TE_Sequence.fasta.csv Metrics on test set are: \n",
    "----Confusion Matrix:\n",
    "[[ 66  77]\n",
    " [174 229]]\n",
    "----Matthews Corrcoef:\n",
    "0.026377304180641305\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "  No crystal       0.28      0.46      0.34       143\n",
    "     Crystal       0.75      0.57      0.65       403\n",
    "\n",
    "    accuracy                           0.54       546\n",
    "   macro avg       0.51      0.51      0.50       546\n",
    "weighted avg       0.62      0.54      0.57       546\n",
    "\n",
    "-------CRYS_TE_Sequence.fasta.csv Metrics on test set are: \n",
    "----Confusion Matrix:\n",
    "[[3647  979]\n",
    " [ 276   45]]\n",
    "----Matthews Corrcoef:\n",
    "-0.04343660694468044\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "  No crystal       0.93      0.79      0.85      4626\n",
    "     Crystal       0.04      0.14      0.07       321\n",
    "\n",
    "    accuracy                           0.75      4947\n",
    "   macro avg       0.49      0.46      0.46      4947\n",
    "weighted avg       0.87      0.75      0.80      4947\n",
    "\n",
    "-------MC_TE_Sequence.fasta.csv Metrics on test set are: \n",
    "----Confusion Matrix:\n",
    "[[891   0]\n",
    " [129   0]]\n",
    "----Matthews Corrcoef:\n",
    "0.0\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "  No crystal       0.87      1.00      0.93       891\n",
    "     Crystal       0.00      0.00      0.00       129\n",
    "\n",
    "    accuracy                           0.87      1020\n",
    "   macro avg       0.44      0.50      0.47      1020\n",
    "weighted avg       0.76      0.87      0.81      1020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23b43520f3e5455da3558597cfdc7903efc679b2ebba7268f9329257f527d575"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
